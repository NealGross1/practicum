{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "#https://github.com/salesforce/LAVIS\n",
    "from lavis.models import load_model_and_preprocess\n",
    "from utils.coco_utils.annotate_images import annotate_img\n",
    "from time import time\n",
    "import os \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neals\\anaconda3\\envs\\practicum\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('loading model')\n",
    "model, vis_processors, txt_processors  = load_model_and_preprocess(name=\"blip2_feature_extractor\", model_type=\"pretrain\", is_eval=True, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {\"image\": None, \"text_input\": ['A sattellite image of a plane']}\n",
    "sample2 = {\"image\": None, \"text_input\": ['A sattellite image of a ship']}\n",
    "features_text1 = model.extract_features(sample, mode=\"text\")\n",
    "features_text2 = model.extract_features(sample2, mode=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlipOutputFeatures(image_embeds=None, image_embeds_proj=None, text_embeds=tensor([[[-0.0419,  0.0328,  0.1032,  ...,  0.0038,  0.3726,  0.1698],\n",
       "         [-0.2967,  0.0822,  0.4625,  ..., -0.0868,  0.8165,  0.4379],\n",
       "         [ 0.3968,  0.0216,  0.9961,  ..., -0.0717,  0.6900,  0.5643],\n",
       "         ...,\n",
       "         [-0.4789, -0.3678,  0.7024,  ..., -0.2755,  0.8651,  0.5216],\n",
       "         [-0.1453, -0.0287,  0.4917,  ..., -0.4297,  1.7845,  0.1681],\n",
       "         [ 0.0681,  0.3537, -0.0088,  ...,  0.5928,  0.1952, -0.1014]]],\n",
       "       device='cuda:0'), text_embeds_proj=tensor([[[-0.0088, -0.0744, -0.0756,  ..., -0.0123, -0.0072,  0.0774],\n",
       "         [ 0.0232, -0.1133, -0.0526,  ...,  0.0543, -0.0306,  0.0679],\n",
       "         [ 0.0421, -0.0692, -0.1240,  ...,  0.0159, -0.0146, -0.0422],\n",
       "         ...,\n",
       "         [ 0.0231, -0.0722, -0.0547,  ...,  0.0809, -0.0653,  0.0302],\n",
       "         [ 0.1010, -0.0583, -0.0315,  ..., -0.0014, -0.0018,  0.0293],\n",
       "         [-0.1161,  0.0036, -0.0813,  ..., -0.1057,  0.0155,  0.1073]]],\n",
       "       device='cuda:0'), multimodal_embeds=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_text1.text_embeds_proj[:,0,:].t().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7799], device='cuda:0')\n",
      "tensor(0.6635, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#similarity = (features_image.image_embeds_proj @ features_text.text_embeds_proj[:,0,:].t()).max()\n",
    "#similarity = F.cosine_similarity(features_text1.text_embeds_proj[:,0,:].t(), features_text2.text_embeds_proj[:,0,:].t())\n",
    "similarity = F.cosine_similarity(features_text1.text_embeds_proj[:,0,:], features_text2.text_embeds_proj[:,0,:])\n",
    "print(similarity)  # This should give a scalar value\n",
    "euclidean_dist = torch.norm(features_text1.text_embeds_proj[:,0,:].t() - features_text2.text_embeds_proj[:,0,:].t())\n",
    "\n",
    "print(euclidean_dist)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_text1.text_embeds_proj[:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8689], device='cuda:0')\n",
      "tensor(0.5120, device='cuda:0')\n",
      "tensor([0.5677], device='cuda:0')\n",
      "tensor(0.9298, device='cuda:0')\n",
      "tensor([0.5126], device='cuda:0')\n",
      "tensor(0.9873, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sample = {\"image\": None, \"text_input\": ['''The image appears to be an aerial or satellite view of a landscape that includes various elements. Here's a detailed description of each visible item:\n",
    "1. **Buildings**: There are several structures visible in the image. On the top left, there is a large building with a gray roof. Below this, there are two smaller structures, one with a gray roof and another with a darker, possibly brown roof.\n",
    "2. **Vehicles**: There are a few vehicles visible. Near the bottom of the image, there is a white vehicle and a darker colored vehicle parked next to a building.\n",
    "3. **Roads and Pathways**: A road or pathway runs vertically down the center of the image, leading to the buildings and parking areas.\n",
    "4. **Vegetation**: The area is heavily vegetated with various shades of green, indicating different types of trees and possibly some grassy areas. There is a dense cluster of trees in the center of the image.\n",
    "5. **Bare Land**: There is a large patch of bare land or possibly a field with very short vegetation in the center, surrounded by the trees.\n",
    "6. **Miscellaneous Debris or Materials**: Scattered around the buildings and open land are various patches that could be debris, materials, or disturbed soil.\n",
    "This image provides a comprehensive view of a rural or semi-rural property with residential and possibly agricultural or storage uses.''']}\n",
    "sample2 = {\"image\": None, \"text_input\": ['''Describe every item in the image,\"The image appears to be an aerial view of a residential area with various items visible. Here's a description of each item seen in the image:\n",
    "1. **Houses**: There are two houses visible, one with a gray roof and another with a brown roof.\n",
    "2. **Boat**: A small boat is docked on the water near a pier.\n",
    "3. **Pier**: A small pier extends into the water, with a blue and white structure at the end.\n",
    "4. **Garden or Flower Bed**: There is a colorful area that looks like a garden or flower bed near the house with the gray roof.\n",
    "5. **Trees and Shrubs**: Various trees and shrubs are scattered throughout the yard areas, providing greenery and shade.\n",
    "6. **Lawn Areas**: There are open grassy areas around the houses.\n",
    "7. **Driveway**: A driveway is visible leading up to the house with the brown roof.\n",
    "8. **Outdoor Structures**: There is a small, round, white structure that could be a storage shed or a playhouse in the yard of the house with the gray roof.\n",
    "9. **Fence or Boundary**: A line of bushes or a low fence appears to demarcate the boundary between the two properties.\n",
    "These elements together create a residential scene with typical backyard features.''']}\n",
    "sample3 = {\"image\": None, \"text_input\": ['''The image appears to be a satellite view of a ship in a harbor''']}\n",
    "features_text1 = model.extract_features(sample, mode=\"text\")\n",
    "features_text2 = model.extract_features(sample2, mode=\"text\")\n",
    "features_text3 = model.extract_features(sample3, mode=\"text\")\n",
    "similarity = F.cosine_similarity(features_text1.text_embeds_proj[:,0,:], features_text2.text_embeds_proj[:,0,:])\n",
    "print(similarity)  # This should give a scalar value\n",
    "euclidean_dist = torch.norm(features_text1.text_embeds_proj[:,0,:].t() - features_text2.text_embeds_proj[:,0,:].t())\n",
    "print(euclidean_dist) \n",
    "\n",
    "similarity = F.cosine_similarity(features_text1.text_embeds_proj[:,0,:], features_text3.text_embeds_proj[:,0,:])\n",
    "print(similarity)  # This should give a scalar value\n",
    "euclidean_dist = torch.norm(features_text1.text_embeds_proj[:,0,:].t() - features_text3.text_embeds_proj[:,0,:].t())\n",
    "print(euclidean_dist)  \n",
    " \n",
    "similarity = F.cosine_similarity(features_text2.text_embeds_proj[:,0,:], features_text3.text_embeds_proj[:,0,:])\n",
    "print(similarity)  # This should give a scalar value\n",
    "euclidean_dist = torch.norm(features_text2.text_embeds_proj[:,0,:].t() - features_text3.text_embeds_proj[:,0,:].t())\n",
    "print(euclidean_dist)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
